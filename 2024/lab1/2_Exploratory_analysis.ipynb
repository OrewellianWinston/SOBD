{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа № 1 \n",
    "## Выполнение разведочного анализа больших данных с использованием фреймворка Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2\n",
    "\n",
    "В данной части работы рассмотрены:\n",
    "* разведочный анализ данных;\n",
    "* работа с Dataframe API фреймворка `Apache Spark`.\n",
    "\n",
    "Подключаем необходимые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, sum, mean, when,\n",
    "    explode, count, desc, floor,\n",
    "    corr, array_contains, lit, first\n",
    ")\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем объект конфигурации для `Apache Spark`, указав необходимые параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_configuration() -> SparkConf:\n",
    "    \"\"\"\n",
    "    Создает и конфигурирует экземпляр SparkConf для приложения Spark.\n",
    "\n",
    "    Returns:\n",
    "        SparkConf: Настроенный экземпляр SparkConf.\n",
    "    \"\"\"\n",
    "    # Получаем имя пользователя\n",
    "    user_name = os.getenv(\"USER\")\n",
    "    \n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(\"lab 1 Test\")\n",
    "    conf.setMaster(\"yarn\")\n",
    "    conf.set(\"spark.submit.deployMode\", \"client\")\n",
    "    conf.set(\"spark.executor.memory\", \"12g\")\n",
    "    conf.set(\"spark.executor.cores\", \"8\")\n",
    "    conf.set(\"spark.executor.instances\", \"2\")\n",
    "    conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    conf.set(\"spark.driver.cores\", \"2\")\n",
    "    conf.set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.0\")\n",
    "    conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog.warehouse\", f\"hdfs:///user/{user_name}/warehouse\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\")\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём сам объект конфигурации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = create_spark_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём и выводим на экран сессию `Apache Spark`. В процессе создания сессии происходит подключение к кластеру `Apache Hadoop`, что может занять некоторое время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Укажем базу данных, которая была создана в первой части лабораторной работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"ivanov_database\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим созданную базу данных как текущую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем сохранённую в предыдущей части работы таблицу и загрузим её в `Spark Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"sobd_lab1_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем прочитанную таблицу на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на схему данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим количество строк в датафрейме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `vin`\n",
    "\n",
    "Отсортируем датафрейм по столбцу `vin`, который может рассматриваться в качестве первичного ключа таблицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(\"vin\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что некоторые значения ключа довольно странные (представляют собой больше описание, чем ключ). Удалим из датафрейма строки с такими ключами, оставив только подходящие по шаблону."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Регулярное выражение для VIN длиной 17 символов, состоящих из цифр и букв\n",
    "vin_pattern = r\"^[A-Z0-9]{17}$\"\n",
    "\n",
    "# Фильтрация DataFrame\n",
    "df = df.filter(col(\"vin\").rlike(vin_pattern))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим наличие дубликатов в датафрейме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .groupBy(\"vin\")\n",
    "    .count()\n",
    "    .where(\"count > 1\")\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дубликаты есть. Посмотрим, что они собой представляют (на примере одной записи)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col(\"vin\") == \"1G1ZE5SX2LF145812\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похоже на полную идентичность строк. Удалим дубликаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates([\"vin\"])\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `body_type`\n",
    "\n",
    "Посмотрим внимательно на значения в столбце. Видно, что в данном столбце расположен **категориальный признак**.\n",
    "\n",
    "Введем функцию, определяющую количество NULL-значений в столбце."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nulls(data: DataFrame,\n",
    "                column_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Подсчет количества null и not null значений в указанном столбце.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame, содержащий данные.\n",
    "        column_name (str): Имя столбца для подсчета null и not null значений.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Подсчет количества null значений в указанном столбце\n",
    "    null_counts = data.select(\n",
    "        sum(col(column_name).isNull().cast(\"int\"))\n",
    "    ).collect()[0][0]\n",
    "\n",
    "    # Подсчет количества not null значений в указанном столбце\n",
    "    not_null_counts = data.select(\n",
    "        sum(col(column_name).isNotNull().cast(\"int\"))\n",
    "    ).collect()[0][0]\n",
    "\n",
    "    # Вывод результатов\n",
    "    print(f\"Число колонок с NULL: {null_counts} \"\n",
    "          f\"({100 * null_counts / (null_counts + not_null_counts):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"body_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что столбец `body_type` содержит небольшое количество пропущенных значений. Поскольку признак содержит категорию (тип кузова автомобиля), то логично заменить пропущенные значения на категорию `Unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({\"body_type\": \"Unknown\"})\n",
    "count_nulls(data=df, column_name=\"body_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию расчета и визуализации распределения категориальных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cat_distribution(data: DataFrame,\n",
    "                          column_name: str,\n",
    "                          top_n: int = 20) -> None:\n",
    "    \"\"\"\n",
    "    Построение распределения категориального признака.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame, содержащий данные.\n",
    "        column_name (str): Имя столбца для группировки.\n",
    "        top_n (int): Количество топ-значений для отображения.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Группировка данных по столбцу и подсчет количества\n",
    "    categories = (\n",
    "        data\n",
    "        .groupBy(column_name)\n",
    "        .count()\n",
    "        .orderBy(\"count\", ascending=False)\n",
    "    )\n",
    "    \n",
    "    print(f\"Количество категорий признака {column_name}: {categories.count()}\")\n",
    "\n",
    "    categories = (\n",
    "        categories\n",
    "        .limit(top_n)\n",
    "        .toPandas()\n",
    "    )\n",
    "    \n",
    "    # Визуализация с использованием Seaborn\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=column_name, y=\"count\", data=categories)\n",
    "    plt.title(f\"Barplot of \\\"{column_name}\\\" counts\")\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_distribution(data=df, column_name=\"body_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что автомобили в представленном датасете имеют 9 определенных типов кузова, а для части автомобилей тип кузова неизвестен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `daysonmarket`\n",
    "\n",
    "В соответствии с описанием и содержанием датасета логично считать данный признак **количественным**. Проверим его на наличие пропущенных значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"daysonmarket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что пропуски в данном столбце отсутствуют.\n",
    "\n",
    "Создадим функцию, позволяющую рассчитывать статистические показатели данных в столбцах и строить диаграмму \"ящик с усами\" для оценки наличия выбросов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(data: DataFrame,\n",
    "                  columns: list[str],\n",
    "                  sample_fraction: float = 0.1) -> None:\n",
    "    \"\"\"\n",
    "    Построение boxplot для нескольких столбцов в PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame, содержащий данные.\n",
    "        columns (list of str): Список имен столбцов для построения boxplot.\n",
    "        sample_fraction (float): Доля данных для семплирования выбросов.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    box_data = []\n",
    "\n",
    "    for column in columns:\n",
    "        # Вычисление квантилей\n",
    "        quantiles = data.approxQuantile(column, [0.25, 0.5, 0.75], 0.01)\n",
    "        q1, median, q3 = quantiles\n",
    "\n",
    "        # Вычисление IQR и границ усов\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "        # Фильтрация выбросов\n",
    "        filtered_df = data.filter((col(column) >= lower_bound) & (col(column) <= upper_bound))\n",
    "        outliers_df = data.filter((col(column) < lower_bound) | (col(column) > upper_bound))\n",
    "\n",
    "        # Вычисление минимального и максимального значений\n",
    "        min_value = data.agg({column: \"min\"}).collect()[0][0]\n",
    "        mean_value = data.agg({column: \"mean\"}).collect()[0][0]\n",
    "        std_value = data.agg({column: \"std\"}).collect()[0][0]\n",
    "        max_value = data.agg({column: \"max\"}).collect()[0][0]\n",
    "\n",
    "        # Ограничение усов минимальным и максимальным значениями\n",
    "        lower_bound = max(lower_bound, min_value)\n",
    "        upper_bound = min(upper_bound, max_value)\n",
    "\n",
    "        # Семплирование выбросов\n",
    "        outliers = []\n",
    "        if not outliers_df.isEmpty():\n",
    "            sampled_outliers_df = outliers_df.sample(sample_fraction)\n",
    "            outliers = (\n",
    "                sampled_outliers_df\n",
    "                .select(column)\n",
    "                .limit(1000)\n",
    "                .collect()\n",
    "            )\n",
    "            outliers = [row[column] for row in outliers]\n",
    "            \n",
    "            # Добавление минимального и максимального значений, если они \n",
    "            # относятся к выбросам и не присутствуют в семпле\n",
    "            if min_value < lower_bound and min_value not in outliers:\n",
    "                outliers.append(min_value)\n",
    "            if max_value > upper_bound and max_value not in outliers:\n",
    "                outliers.append(max_value)\n",
    "\n",
    "        # Подготовка данных для axes.bxp\n",
    "        box_data.append({\n",
    "            'whislo': lower_bound,  # Нижняя граница усов\n",
    "            'q1': q1,               # Первый квартиль\n",
    "            'med': median,          # Медиана\n",
    "            'q3': q3,               # Третий квартиль\n",
    "            'whishi': upper_bound,  # Верхняя граница усов\n",
    "            'fliers': outliers      # Выбросы\n",
    "        })\n",
    "        \n",
    "    # Вывод статистических характеристик\n",
    "    print(f\"Минимальное значение:          {min_value:.2f}\")\n",
    "    print(f\"Среднее значение:              {mean_value:.2f}\")\n",
    "    print(f\"Среднеквадратичное отклонение: {std_value:.2f}\")\n",
    "    print(f\"Первый квартиль:               {q1:.2f}\")\n",
    "    print(f\"Медиана:                       {median:.2f}\")\n",
    "    print(f\"Третий квартиль:               {q3:.2f}\")\n",
    "    print(f\"Максимальное значение:         {max_value:.2f}\")\n",
    "\n",
    "    # Построение boxplot\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    ax.bxp(box_data, \n",
    "           vert=False, \n",
    "           positions=range(1, len(columns) + 1), widths=0.5)\n",
    "    ax.set_yticks(range(1, len(columns) + 1))\n",
    "    ax.set_yticklabels(columns)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_title('Boxplots')\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(data=df, columns=[\"daysonmarket\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наблюдаем значительное количество выбросов в данных. Для более тщательного исследования создадим функцию для визуализации распределения категориального признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quant_distribution(data: DataFrame,\n",
    "                            column: str,\n",
    "                            num_bins: int = 200) -> None:\n",
    "    \"\"\"\n",
    "    Построение гистограммы для количественной переменной с \n",
    "    использованием PySpark и Seaborn.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame с данными.\n",
    "        column_name (str): Название колонки с количественной переменной.\n",
    "        num_bins (int): Количество бинов для гистограммы.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Находим минимальное и максимальное значения колонки\n",
    "    min_value = data.agg({column: \"min\"}).collect()[0][0]\n",
    "    max_value = data.agg({column: \"max\"}).collect()[0][0]\n",
    "\n",
    "    # Размер бина\n",
    "    bin_size = (max_value - min_value) / num_bins\n",
    "\n",
    "    # Добавляем колонку с номером бина\n",
    "    data = data.withColumn(\n",
    "        \"bin\", \n",
    "        floor((col(column) - min_value) / bin_size)\n",
    "    )\n",
    "\n",
    "    # Группируем по номеру бина и считаем количество строк в каждом бине\n",
    "    bin_counts = data.groupBy(\"bin\").count()\n",
    "\n",
    "    # Преобразуем результат в Pandas DataFrame для построения гистограммы\n",
    "    bin_counts_pd = bin_counts.limit(1000).toPandas()\n",
    "    \n",
    "    # Создаем массив границ бинов\n",
    "    bin_edges = [min_value + i * bin_size for i in range(num_bins + 2)]\n",
    "    \n",
    "    # Преобразуем номера бинов в центры бинов\n",
    "    bin_centers = [\n",
    "        (bin_edges[i] + bin_edges[i + 1]) / 2 for i in range(num_bins + 1)\n",
    "    ]\n",
    "    \n",
    "    # Добавляем центры бинов в Pandas DataFrame\n",
    "    bin_counts_pd['bin_center'] = bin_counts_pd['bin'].apply(\n",
    "        lambda x: bin_centers[int(x)]\n",
    "    )\n",
    "    \n",
    "    # Построение гистограммы с использованием Seaborn\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    sns.histplot(data=bin_counts_pd, x=\"bin_center\", \n",
    "                 weights=\"count\", kde=True, bins=num_bins + 1)\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Распределение количественного признака \\\"{column}\\\"\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quant_distribution(data=df, column=\"daysonmarket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что почти все данные не превышают значения 500, но при этом наблюдается малое количество довольно сильных выбросов. Обрежем эти выбросы, установив для них максимальную границу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"daysonmarket\",\n",
    "    when(col(\"daysonmarket\") > 500.0, 500.0)\n",
    "        .otherwise(col(\"daysonmarket\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quant_distribution(data=df, column=\"daysonmarket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь диаграмма более эффективно представляет данные в столбце."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `fleet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"fleet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что более половины данных в столбце пропущены. Можно, конечно, попытаться обработать то, что есть, но в целях упрощения анализа просто удалим столбец."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"fleet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `has_accidents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"has_accidents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, ситуация аналогичная. Удаляем столбец."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"has_accidents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `horsepower`\n",
    "\n",
    "Данный столбец, согласно описанию и значениям, которые он принимает, можно отнести к **количественным**.\n",
    "\n",
    "Выполним аналогичные шаги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"horsepower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропущено много значений, но меньше половины. Стоит попытаться сохранить признак и обработать его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(data=df, columns=[\"horsepower\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно наблюдать сильные выбросы. Удалим строки, их содержащие, и убедимся, что потеряна небольшая часть данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col(\"horsepower\") > 400).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим пропуски средним значением признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(col(\"horsepower\") < 400)\n",
    "mean_horsepower = df.select(mean(col(\"horsepower\"))).collect()[0][0]\n",
    "mean_horsepower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({\"horsepower\": mean_horsepower})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quant_distribution(data=df, column=\"horsepower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбцов `is_certified, is_cpo, is_oemcpo`\n",
    "\n",
    "Данные признаки являются **бинарными** и имеют схожую интерпретацию. Заменим их одним признаком, который принимает значение `True`, если хотя бы один из вышеупомянитых признаков содержит истинное значение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"is_any_cert\", \n",
    "    when(\n",
    "        col(\"is_certified\") | col(\"is_cpo\") | col(\"is_oemcpo\"), \n",
    "        True\n",
    "    ).otherwise(False)\n",
    ").drop(col(\"is_certified\")).drop(col(\"is_cpo\")).drop(col(\"is_oemcpo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"is_any_cert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропуски отсутствуют."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_distribution(data=df, column_name=\"is_any_cert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ признака `major_options`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"major_options\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропуски есть, но их немного. Заменим их пустыми списками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"major_options\", \n",
    "    when(\n",
    "        col(\"major_options\").isNull(), \n",
    "        lit([])\n",
    "    ).otherwise(col(\"major_options\"))\n",
    ")\n",
    "\n",
    "count_nulls(data=df, column_name=\"major_options\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный признак представляет собой **массив признаков**. Обработаем его следующим образом. Определим пятёрку самых часто встречающихся элементов списка для всех объектов из датасета и введем пять **бинарных** признаков, показывающих присутствие элемента в списке конкретного объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popular_options(data: DataFrame, \n",
    "                        column: str, \n",
    "                        top_n: int = 5) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Получение DataFrame всех элементов массива опций, подсчет их количества, \n",
    "    сортировка по убыванию и вывод нескольких самых популярных элементов.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame, содержащий данные.\n",
    "        column (str): Имя столбца с массивом.\n",
    "        top_n (int): Количество самых популярных элементов для вывода.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame с самыми популярными элементами.\n",
    "    \"\"\"\n",
    "    # Развертывание массива в отдельные строки\n",
    "    exploded_df = data.withColumn(\"element\", explode(col(column)))\n",
    "\n",
    "    # Подсчет количества каждого элемента\n",
    "    element_counts = (\n",
    "        exploded_df\n",
    "        .groupBy(\"element\")\n",
    "        .agg(count(\"element\").alias(\"count\"))\n",
    "    )\n",
    "\n",
    "    # Сортировка элементов по убыванию\n",
    "    sorted_elements = element_counts.orderBy(desc(\"count\"))\n",
    "\n",
    "    # Вывод нескольких самых популярных элементов\n",
    "    top_elements = sorted_elements.limit(top_n)\n",
    "\n",
    "    return top_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим датафрейм из пяти самых популярных опций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_options = get_popular_options(data=df, column=\"major_options\")\n",
    "popular_options.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_options_columns(data: DataFrame,\n",
    "                        column: str,\n",
    "                        popular_options: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Добавление новых булевых колонок в DataFrame, указывающих, \n",
    "    содержится ли определённый элемент в каждой строке.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame, содержащий данные.\n",
    "        column (str): Имя столбца с массивом.\n",
    "        popular_elements (DataFrame): DataFrame с популярными элементами.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame с новыми булевыми колонками.\n",
    "    \"\"\"\n",
    "    # Добавление булевых колонок для каждой популярной категории\n",
    "    categories_df = popular_options.select(col(\"element\").alias(\"category\"))\n",
    "    result = data.crossJoin(categories_df.hint(\"broadcast\"))\n",
    "    result = result.withColumn(\"contains\", \n",
    "                               array_contains(col(column), col(\"category\")))\n",
    "    result = result.groupBy(data.columns).pivot(\"category\").agg(first(\"contains\"))\n",
    "    \n",
    "    for col_name in result.columns:\n",
    "        if col_name not in data.columns:\n",
    "            result = result.withColumnRenamed(col_name, f\"contains_{col_name}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним вышеописанное преобразование над датасетом, а признак-массив удалим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_options_columns(data=df, column=\"major_options\", \n",
    "                         popular_options=popular_options)\n",
    "df = df.drop(\"major_options\").cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `maximum_seating`\n",
    "\n",
    "Данный признак является **количественным**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"maximum_seating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .filter(col(\"maximum_seating\").isNotNull())\n",
    "    .groupBy(\"maximum_seating\")\n",
    "    .count()\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим только те автомобили, число мест в которых менее 20. Их преобладающее количество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(col(\"maximum_seating\") < 20)\n",
    "mean_maximum_seating = int(\n",
    "    df.select(mean(col(\"maximum_seating\"))).collect()[0][0]\n",
    ")\n",
    "mean_maximum_seating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим пропуски средним значением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({\"maximum_seating\": mean_maximum_seating})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_distribution(data=df, column_name=\"maximum_seating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ признака `price`\n",
    "\n",
    "Признак **количественный**. Все преобразования аналогичны вышерассмотренным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(data=df, columns=[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col(\"price\") > 60000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(col(\"price\") <= 60000)\n",
    "plot_quant_distribution(data=df, column=\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `wheel_system`\n",
    "\n",
    "Признак **категориальный**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"wheel_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .filter(col(\"wheel_system\").isNotNull())\n",
    "    .groupBy(\"wheel_system\")\n",
    "    .count()\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим пропуски модой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wheel_system_mode = (\n",
    "    df\n",
    "    .filter(col(\"wheel_system\").isNotNull())\n",
    "    .groupBy(\"wheel_system\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    .first()[0]\n",
    ")\n",
    "wheel_system_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({\"wheel_system\": wheel_system_mode})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_distribution(data=df, column_name=\"wheel_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `mileage`\n",
    "\n",
    "Признак **количественный**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"mileage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(col(\"mileage\").isNotNull())\n",
    "count_nulls(data=df, column_name=\"mileage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(data=df, columns=[\"mileage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quant_distribution(data=df, column=\"mileage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(col(\"mileage\") <= 200000.0)\n",
    "plot_quant_distribution(data=df, column=\"mileage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ признака `year`\n",
    "\n",
    "Признак **количественный**.\n",
    "\n",
    "Преобразуем год выпуска в количество лет с года выпуска до 2024 года."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"age\", lit(2024) - col(\"year\")).drop(\"year\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Расчет корреляции между количественными признаками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_visualize_correlation_matrix(data: DataFrame, \n",
    "                                             columns: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Вычисляет и визуализирует корреляционную матрицу для указанных \n",
    "    колонок в DataFrame PySpark.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame PySpark.\n",
    "        columns (list[str]): Список колонок для вычисления корреляции.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Вычисление корреляционной матрицы\n",
    "    corr_matrix = {}\n",
    "    for col1 in columns:\n",
    "        corr_matrix[col1] = {}\n",
    "        for col2 in columns:\n",
    "            corr_value = data.select(corr(col1, col2)).collect()[0][0]\n",
    "            corr_matrix[col1][col2] = corr_value\n",
    "\n",
    "    # Преобразование корреляционной матрицы в DataFrame Pandas для визуализации\n",
    "    corr_matrix_pd = pd.DataFrame(corr_matrix)\n",
    "\n",
    "    # Построение и визуализация корреляционной матрицы\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix_pd, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_and_visualize_correlation_matrix(\n",
    "    data=df, columns=[\n",
    "        \"daysonmarket\", \"horsepower\", \"maximum_seating\",\n",
    "        \"mileage\", \"age\", \"price\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корреляционная матрица демонстрирует наличие корреляции между некоторыми количественными признаками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, сколько объектов осталось после преобразований датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняет очищенную и обработанную таблицу на диск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение DataFrame в виде таблицы\n",
    "df.writeTo(\"sobd_lab1_processed_table\").using(\"iceberg\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in spark.catalog.listTables():\n",
    "    print(table.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что при необходимости созданные базу данных и таблицу можно удалить следующими командами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"DROP TABLE spark_catalog.ivanov_database.sobd_lab1_processed_table\")\n",
    "# spark.sql(\"DROP DATABASE spark_catalog.ivanov_database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Останавливаем `Spark`-сессию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
