## Лабораторная работа № 1
### Выполнение разведочного анализа больших данных с использованием фреймворка Apache Spark
#### Цель работы:
* Выполнить разведочный анализ датасета большого объёма с помощью фреймворка `Apache Spark`.

#### Задачи работы:
1. Познакомиться с понятием «большие данные» и способами их обработки.
2. Познакомиться с инструментом `Apache Spark` и возможностями, которые он предоставляет для обработки больших данных.
3. Получить представление об инструментах экосистемы `Hadoop`: HDFS и YARN.
4. Поработать с табличным форматом для больших данных `Apache Iceberg`.
4. Получить навыки выполнения разведочного анализа данных использованием `pyspark`.

#### Порядок выполнения работы:
1. Установите среду разработки [Microsoft Visual Studio Code](https://code.visualstudio.com/).
2. Запустите `VS Code`, на левой панели откройте вкладку `Extensions` и установите расширения `Python`, `Jupyter`, `Remote - SSH`.
3. На левой панели откройте вкладку `Remote Explorer`, наведите мышкой на строку `SSH` и щелкните знак `+`.
4. В открывшейся строке наберите следующую команду, заменив `user`, `ip` и `port` на параметры учетной записи. Учетные записи на кластер доступны в ЭИОС.

`ssh -J user@ip:port user@hadoop-node`

После введения строки укажите файл `~/.ssh/config` в качестве хранилища записи.

5. Щелкните по кнопке с изображением стрелки на появившемся хосте. Если VS Code запросит платформу удалённого узла, укажите `Linux`.
   
6. При запросе пароля от учетной введите его (возожно, дважды). Дождитесь соединения с кластером и настройки удалённого сервера.
   
7. Установите на удаленный сервер расширения `Python` и `Jupyter` тем же способом, что и ранее (убедитесь, что рядом с именем расширения указано `Install in SSH: hadoop-node`)
   
8. Нажмите на вкладку `Terminal` на панели инструментов `VS Code` и откройте новый терминал.
   
9. Проверьте доступность файловой системы `HDFS`, набрав команду:

`hdfs dfs -ls /`

Побродите по файловой системе `HDFS`, обратите внимание на права доступа к файлам и каталогам.

Проверьте объём оставшегося дискового пространства `HDFS`, набрав команду:

`hdfs dfs -df -h`

10. Проверьте доступность системы `YARN` командой:

`yarn node -list`

11. Проверьте загрузку очереди `YARN`, набрав команду:

`yarn application -list`

Ненужное приложение можно остановить командой:

`yarn application -kill <app-id>`

12. Создайте свою директорию в локальной директории пользователя на кластере и перейдите в неё:

`mkdir ivanov_directory`

`cd ivanov_directory`

13. Склонируйте репозиторий с лабораторными работами:

`git clone https://github.com/kpdvstu/SOBD.git`

14. На левой панели `VS Code` нажмите на кнопку `Open Folder` и перейдите в директорию с первой лабораторной работой.
    
15. Откройте файлы `Jupyter Notebook` (*.ipynb), выберите виртуальное окружение `pyspark-env` и поочерёдно запустите инструкции в файлах на исполнение. Некоторые инструкции потребуют модификации информации о запустившем их пользователе, не пропустите их.
    
* **Совет**. В процессе работы с ноутбуком удобно запустить в терминале команду, позволяющую отслеживать в реальном времени все работающие задачи в `YARN`:

`watch yarn application -list`

16. **Выполните аналогичным образом разведочный анализ датасета по Вашему выбору** с определением:
* типов признаков в датасете;
* пропущенных значений и их устранением;
* выбросов и их устранением;
* расчетом статистических показателей признаков (средних, квартилей и т.д.);
* визуализацией распределения наиболее важных признаков;
* корреляций между признаками.

**Обратите внимание**:
* Объём датасета -- не менее **нескольких гигабайт**.
* Датасеты можно выбрать на [Kaggle](https://www.kaggle.com/datasets) (для скачивания датасета требуется регистрация).

17. Сделайте выводы по работе.
18. Напишите **главу курсовой работы** в пределах **15-20 страниц**, в которой опишите постановку задачи, описание Вашего датасета со ссылкой на него, проведенный разведовательный анализ и выводы.
19. Сохраните ноутбук с проведенным анализом и написанную главу курсовой в `GitHub / GitLab`.

**К отчету** следует представить репозиторий на `GitHub / GitLab` с выполненным разведочным анализом и главой курсовой работы, а также быть готовым продемонстрировать работоспособность кода и пояснить спорные моменты.

#### Список теоретических вопросов к отчету:

### Список вопросов будет пополняться и изменяться после лекций!

1. Фреймворк обработки больших данных `Apache Spark`, его назначение, функции и отличия от `Hadoop MapReduce`.
2. Понятие устойчивого распределенного набора данных (RDD). Понятие раздела RDD (partition). Способы создания RDD. Трансформации (transformations) и действия (actions). Кэширование (cache) данных в Spark.
3. RDD и PairRDD: понятие, назначение. Основные трансформации (transformations) и действия (actions) над ними.
4. Реализация концепции MapReduce в фреймворке Spark. Функции map, flatMap, mapValues, mapPartitions, reduce, reduceByKey.
5. Модели запуска Spark-приложений (YARN, Standalone, Kubernetes). Понятие драйвера (driver) и исполнителей (executors). Понятия задания (job), этапа (stage) и задачи (task). Модель ленивых вычислений (lazy) и ее применение в Spark. Понятие ориентированного ациклического графа (Directed Acyclic Graph, DAG).
6. Операция перемешивания данных (shuffle): причины возникновения, влияние на производительность. Класс Partitioner. Пути повышения производительности.
7. Понятие датафрейма в Spark. Основные операции над датафреймами. Скалярные функции, агрегирующие функции, оконные функции. Оптимизация запросов. Catalyst.
8. Клиентский (client) и кластерный (cluster) режимы работы Apache Spark.

#### Литература для подготовки к отчету:
1. Изучаем Spark: молниеносный анализ данных / Х. Карау, Э. Конвински, П. Венделл, М.М. Захария // ДМК Пресс, 2015. — 304 с.: ил.
2. Data Exploration // Learning Apache Spark with Python [Электронный  ресурс] / W. Feng. - [2021]. - Режим доступа : https://runawayhorse001.github.io/LearningApacheSpark/exploration.html (дата обращ. 19.09.2022).
3. Advanced Pyspark for Exploratory Data Analysis [Электронный  ресурс]. – [2022]. – Режим доступа : https://www.kaggle.com/code/tientd95/advanced-pyspark-for-exploratory-data-analysis (дата обращ. 19.09.2022).
4. Exploratory Data Analysis (EDA) with PySpark on Databricks [Электронный  ресурс]. – [2020]. – Режим доступа : https://towardsdatascience.com/exploratory-data-analysis-eda-with-pyspark-on-databricks-e8d6529626b1 (дата обращ. 19.09.2022).
5. Exploratory data analysis with pySpark [Электронный  ресурс]. – [2020]. – Режим доступа : https://github.com/roshankoirala/pySpark_tutorial/blob/master/Exploratory_data_analysis_with_pySpark.ipynb (дата обращ. 19.09.2022).
6. Официальный сайт Apache Spark [Электронный  ресурс]. – [2022]. – Режим доступа : https://spark.apache.org/ (дата обращ. 19.09.2022).
7. Уайт, Т. Hadoop: Подробное руководство / Т. Уайт. — 3-е изд. — СПб. : Питер, 2013. — 672 с.: ил.
